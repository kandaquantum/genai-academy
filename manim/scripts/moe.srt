1
00:00:00,000 --> 00:00:08,000
Mixture of Experts、略してMOEは、
複数の専門家モデルを組み合わせたアーキテクチャです。

2
00:00:08,000 --> 00:00:14,000
入力データは各エキスパートに渡され、 それぞれが独自の出力を生成します。

3
00:00:14,000 --> 00:00:23,000
同時にゲーティング関数が各専門家モデルの出力を重み付けし、
最終的な出力を決定します。

4
00:00:23,000 --> 00:00:32,000
例えば、可愛い広告バナーを作成する際には、
マーケター、デザイナー、ライターといった専門家が協力します。

5
00:00:32,000 --> 00:00:40,000
各専門家がアイデアを出し合い、ディレクターがそれらを取捨選択し、
最適な広告バナーを生み出すのです。